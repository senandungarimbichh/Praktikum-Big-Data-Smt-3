{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a8205af-d986-4fb4-8046-16408da9ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 21:37:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/12/12 21:37:44 WARN Instrumentation: [24249be8] regParam is zero, which might cause numerical instability and overfitting.\n",
      "[Stage 37:==========================================================(8 + 0) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Hands-on \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder.appName(\"MLlib LinearRegression\").getOrCreate()\n",
    "\n",
    "# Data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Output\n",
    "print(f\"Coefficients: {model.coefficients}\")\n",
    "print(f\"Intercept: {model.intercept}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8927b034-3ea9-466a-b7cc-3aafb469a13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 21:39:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients: [-12.262057915782703,4.087352262022548]\n",
      "Model Intercept: 11.568912713246394\n"
     ]
    }
   ],
   "source": [
    "# Hands-on\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Start Spark\n",
    "spark = SparkSession.builder.appName(\"MLlib LogisticRegression\").getOrCreate()\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, [2.0, 3.0], 0),\n",
    "    (2, [1.0, 5.0], 1),\n",
    "    (3, [2.5, 4.5], 1),\n",
    "    (4, [3.0, 6.0], 0)\n",
    "]\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Split features\n",
    "df = df.withColumn(\"Feature1\", df[\"Features\"][0]) \\\n",
    "       .withColumn(\"Feature2\", df[\"Features\"][1])\n",
    "\n",
    "# Vector assembler\n",
    "assembler = VectorAssembler(inputCols=['Feature1', 'Feature2'], outputCol='FeatureVector')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Model\n",
    "lr = LogisticRegression(featuresCol='FeatureVector', labelCol='Label')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Output\n",
    "print(f\"Model Coefficients: {model.coefficients}\")\n",
    "print(f\"Model Intercept: {model.intercept}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122c2cc3-0376-4222-9e2e-8c0b2d8498df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 21:40:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 118:>                                                        (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Hands-on\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Inisialisasi Spark Session\n",
    "spark = SparkSession.builder.appName(\"KMeans Clustering Example\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, [1.0, 1.0]),\n",
    "    (2, [5.0, 5.0]),\n",
    "    (3, [10.0, 10.0]),\n",
    "    (4, [15.0, 15.0])\n",
    "]\n",
    "columns = ['ID', 'Features']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"Feature1\", col(\"Features\")[0]) \\\n",
    "       .withColumn(\"Feature2\", col(\"Features\")[1])\n",
    "\n",
    "# VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Feature1\", \"Feature2\"],\n",
    "    outputCol=\"FeatureVector\"\n",
    ")\n",
    "df_transformed = assembler.transform(df)\n",
    "# Latih model KMeans\n",
    "kmeans = KMeans(featuresCol=\"FeatureVector\", k=2)\n",
    "model = kmeans.fit(df_transformed)\n",
    "centers = model.clusterCenters()\n",
    "print(f\"Cluster Centers: {centers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e596a04-6b36-45d0-9ec2-eceeb5f808e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/12 21:42:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Data Teratas\n",
      "+-------------------+-------+-----+------+-----+-----+------+------+-----+\n",
      "|               date|     co|   no|   no2|   o3|  so2| pm2_5|  pm10|  nh3|\n",
      "+-------------------+-------+-----+------+-----+-----+------+------+-----+\n",
      "|2020-11-25 01:00:00|2616.88| 2.18|  70.6|13.59|38.62|364.61|411.73|28.63|\n",
      "|2020-11-25 02:00:00|3631.59|23.25| 89.11| 0.33|54.36|420.96|486.21|41.04|\n",
      "|2020-11-25 03:00:00|4539.49|52.75|100.08| 1.11|68.67|463.68|541.95|49.14|\n",
      "|2020-11-25 04:00:00|4539.49|50.96|111.04| 6.44| 78.2|454.81| 534.0|48.13|\n",
      "|2020-11-25 05:00:00|4379.27|42.92| 117.9|17.17|87.74|448.14|529.19|46.61|\n",
      "+-------------------+-------+-----+------+-----+-----+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Skema Dataset\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- co: double (nullable = true)\n",
      " |-- no: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- o3: double (nullable = true)\n",
      " |-- so2: double (nullable = true)\n",
      " |-- pm2_5: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- nh3: double (nullable = true)\n",
      "\n",
      "Jumlah Baris: 18776\n"
     ]
    }
   ],
   "source": [
    "# Homework \n",
    "# Load dataset Delhi AQI\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DelhiAQI-MLlib\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load dataset (gunakan absolute path)\n",
    "df = spark.read.csv(\"file:///home/erlyn/delhi_aqi.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"5 Data Teratas\")\n",
    "df.show(5)\n",
    "\n",
    "print(\"Skema Dataset\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"Jumlah Baris:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08195ddc-5460-4b91-a237-c43398304541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Missing Value per Kolom:\n",
      "+----+---+---+---+---+---+-----+----+---+\n",
      "|date| co| no|no2| o3|so2|pm2_5|pm10|nh3|\n",
      "+----+---+---+---+---+---+-----+----+---+\n",
      "|   0|  0|  0|  0|  0|  0|    0|   0|  0|\n",
      "+----+---+---+---+---+---+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identifikasi data\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "print(\"Jumlah Missing Value per Kolom:\")\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5994b1a9-9615-4f9f-9cb4-ee1d645ae36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-----+------+-----+-----+------+------+-----+\n",
      "|               date|     co|   no|   no2|   o3|  so2| pm2_5|  pm10|  nh3|\n",
      "+-------------------+-------+-----+------+-----+-----+------+------+-----+\n",
      "|2020-11-25 01:00:00|2616.88| 2.18|  70.6|13.59|38.62|364.61|411.73|28.63|\n",
      "|2020-11-25 02:00:00|3631.59|23.25| 89.11| 0.33|54.36|420.96|486.21|41.04|\n",
      "|2020-11-25 03:00:00|4539.49|52.75|100.08| 1.11|68.67|463.68|541.95|49.14|\n",
      "|2020-11-25 04:00:00|4539.49|50.96|111.04| 6.44| 78.2|454.81| 534.0|48.13|\n",
      "|2020-11-25 05:00:00|4379.27|42.92| 117.9|17.17|87.74|448.14|529.19|46.61|\n",
      "+-------------------+-------+-----+------+-----+-----+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isi missing value dengan mean tiap kolom\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Mengisi missing value dengan mean dari setiap kolom numerik\n",
    "impute_values = df.select([avg(c).alias(c) for c in df.columns if c != \"date\"]).collect()[0].asDict()\n",
    "\n",
    "df_clean = df.fillna(impute_values)\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f7b107d-b676-435b-9d4f-01b3b0f8c5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------+\n",
      "|label|features                                             |\n",
      "+-----+-----------------------------------------------------+\n",
      "|1    |[2616.88,2.18,70.6,13.59,38.62,364.61,411.73,28.63]  |\n",
      "|1    |[3631.59,23.25,89.11,0.33,54.36,420.96,486.21,41.04] |\n",
      "|1    |[4539.49,52.75,100.08,1.11,68.67,463.68,541.95,49.14]|\n",
      "|1    |[4539.49,50.96,111.04,6.44,78.2,454.81,534.0,48.13]  |\n",
      "|1    |[4379.27,42.92,117.9,17.17,87.74,448.14,529.19,46.61]|\n",
      "+-----+-----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering (VectorAssembler)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# buat label klasifikasi sederhana\n",
    "df_ml = df_clean.withColumn(\"label\", when(col(\"pm2_5\") >= 300, 1).otherwise(0))\n",
    "\n",
    "# daftar fitur\n",
    "feature_cols = [\"co\", \"no\", \"no2\", \"o3\", \"so2\", \"pm2_5\", \"pm10\", \"nh3\"]\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(df_ml)\n",
    "data.select(\"label\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb7b9a2d-0312-4e81-8619-e0f0f1ae3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "co: 0.0725842148854877\n",
      "no: 0.030263975401343346\n",
      "no2: 0.000497750652195045\n",
      "o3: 0.014346907361286054\n",
      "so2: 0.0005795508033445968\n",
      "pm2_5: 0.4187598526263726\n",
      "pm10: 0.4585820690720733\n",
      "nh3: 0.004385679197897377\n"
     ]
    }
   ],
   "source": [
    "# Feature importance (Random Forest)\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(data)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "for col, score in zip(feature_cols, rf_model.featureImportances):\n",
    "    print(f\"{col}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6650879f-9954-4b6a-8ce2-02b91a00aa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------+\n",
      "|label|prediction|probability                 |\n",
      "+-----+----------+----------------------------+\n",
      "|1    |1.0       |[0.0,1.0]                   |\n",
      "|1    |1.0       |[9.308662562765728E-221,1.0]|\n",
      "|0    |0.0       |[1.0,0.0]                   |\n",
      "|1    |1.0       |[0.0,1.0]                   |\n",
      "|1    |1.0       |[1.3762034489540506E-63,1.0]|\n",
      "|0    |0.0       |[1.0,0.0]                   |\n",
      "|0    |0.0       |[1.0,0.0]                   |\n",
      "|0    |0.0       |[1.0,0.0]                   |\n",
      "|0    |0.0       |[1.0,0.0]                   |\n",
      "|0    |0.0       |[1.0,0.0]                   |\n",
      "+-----+----------+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "AUC Score: 0.9999988719645977\n"
     ]
    }
   ],
   "source": [
    "# Klasifikasi\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# split data\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# prediksi\n",
    "pred = lr_model.transform(test)\n",
    "pred.select(\"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# evaluasi model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(pred)\n",
    "print(\"AUC Score:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54d8c7bc-e840-4e8e-a738-8c92ea5eba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Hyperparameters\n",
      "regParam : 0.1\n",
      "elasticNetParam : 1.0\n",
      "AUC dengan Cross Validation: 0.9999996239881992\n"
     ]
    }
   ],
   "source": [
    "# Cross validasi dan hyperparame\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Grid hyperparameter\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# Latih model\n",
    "cv_model = cv.fit(train)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "print(\"Best Model Hyperparameters\")\n",
    "print(\"regParam :\", best_model._java_obj.getRegParam())\n",
    "print(\"elasticNetParam :\", best_model._java_obj.getElasticNetParam())\n",
    "\n",
    "# Evaluasi model terbaik\n",
    "pred_cv = best_model.transform(test)\n",
    "auc_cv = evaluator.evaluate(pred_cv)\n",
    "\n",
    "print(\"AUC dengan Cross Validation:\", auc_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7394383-4602-452f-a0b9-cb6eafa595d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
